batch_size: 16
n_epochs: 1 # TODO: change the number of epochs
learning_rate: 0.001
random_seed: 11
gradient_accumulation_steps: 1 # TODO: change gradient_accumulation_steps
experiment_name: mixture_of_experts
weight_decay: 0.00001
warmup_proportion: 0.1
tokenizer_mask_id: 103
eval_steps: 20 # TODO: change eval_steps
save_steps: 1000 # TODO: change save_steps
save_path: . # TODO: add save_path