batch_size: 32
n_epochs: 5
learning_rate: 0.0005
random_seed: 11
gradient_accumulation_steps: 1 
experiment_name: mixture_of_experts
weight_decay: 0.00001
warmup_proportion: 0.1
tokenizer_mask_id: 103
eval_steps: 1000 
save_steps: 2000
save_path: /train_results